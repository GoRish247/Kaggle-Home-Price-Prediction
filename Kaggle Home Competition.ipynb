{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Setup: Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import re # For regex - not used\n",
    "\n",
    "import matplotlib.pyplot as plt # from matplotlib import pyplot as plt\n",
    "import seaborn as sns # statistical graphics in Python - builds on top of matplotlib \n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "# from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
    "# from mlxtend.regressor import StackingCVRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rgohi\\Documents\\Python Scripts\n"
     ]
    }
   ],
   "source": [
    "#Initial Setup: Load data\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('C:\\\\Users\\\\rgohi\\\\Downloads\\\\')\n",
    "\n",
    "#Read file\n",
    "X_full = pd.read_csv('.\\\\train.csv')\n",
    "X_test_full = pd.read_csv('.\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "da=dir()\n",
    "sys.getsizeof(pd)\n",
    "\n",
    "for val in dir():\n",
    "    print(val, type(eval(val)), sys.getsizeof(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration: Examine data\n",
    "\n",
    "print(type(X_full)) #datatype\n",
    "print(len(X_full)) #number of rows\n",
    "print(X_full.shape) #(row, columns)\n",
    "print(X_full.columns) #column labels\n",
    "print(X_full.dtypes.sort_values(axis=0))\n",
    "print(list(X_full.select_dtypes(include=('float64','int64'))))\n",
    "print(X_full.info())\n",
    "print(test.info(memory_usage='deep')) # Can check out memory usage for larger datasets <- can remove observations/features as needed - such as columns with just one value\n",
    "print(X_full.describe()) #Only displays the numeric columns\n",
    "print(X_full.describe(include=np.object)) #Only displays the categorical/string columns\n",
    "print(X_full[\"LandSlope\"].value_counts()) #Displays counts of each group in a columns\n",
    "print(X_full.loc[X_full[\"LandSlope\"] == \"Sev\",(\"LotArea\",\"SalePrice\")].agg((\"min\",\"max\"))) #Displays mix/max of subsets of columns\n",
    "X_full.head(3)\n",
    "X_full.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration: Visualize data 1 of 2\n",
    "\n",
    "print(X_full.iloc[:,:])\n",
    "# table = pd.pivot_table(data=X_full,index=\"HouseStyle\",values=\"LotArea\",aggfunc=np.average)\n",
    "# print(table)\n",
    "# plt.bar(table.index,table[\"LotArea\"])\n",
    "plt.hist(X_full['YearBuilt'],rwidth=0.9,alpha=0.3,color='blue',bins=15,edgecolor='red') \n",
    "\n",
    "plt.boxplot(X_full[\"YearBuilt\"])\n",
    "# # plt.yticks(range(1, 22))\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "# sns.relplot(data=X_full,y=\"LotArea\",x=\"MSZoning\",hue=\"Condition1\",size=\"GarageArea\") # kind=\"line\" style =\"\" # General Plot\n",
    "print(X_full.columns) #column labels\n",
    "# sns.lmplot(data=X_full,y=\"SalePrice\",x=\"LotArea\",hue=\"MSZoning\") # Linear Regression\n",
    "# ? error sns.displot(data=X_full,x=\"SalePrice\",y=\"LotArea\",kde=\"True\")\n",
    "# ? error sns.catplot(data=X_full,kind=\"violin\", x=\"SaleCondition\", y=\"Street\", hue=\"Street\") #for categorical data #kind=\"bar\"# kind=\"swarm\"\n",
    "\n",
    "#Data PreProcessing: Correlation Matrix & HeatMap\n",
    "\n",
    "a = X_full.corr().nlargest(15,\"SalePrice\")[\"SalePrice\"] #.index #Top \"n\" most correlated features\n",
    "print(a)\n",
    "plt.figure(figsize=(16,6)) # Make plot size larger\n",
    "sns.heatmap(X_full[a.index].corr(),vmin=-1,vmax=1,annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration: Visualize data 1 of 2\n",
    "\n",
    "train_features = X_full.columns\n",
    "Graphing = X_full[['SalePrice']]\n",
    "# X_full['GarageFinish'].hist()\n",
    "# type(X_full['SalePrice']) #series\n",
    "# type(X_full[['SalePrice']]) #dataframe\n",
    "# X_full[['SalePrice']].plot()\n",
    "# X_full['SalePrice'].plot())\n",
    "# X_full[['SalePrice']].plot.area() # or (kind ='area')\n",
    "# Graphing.plot.bar()\n",
    "# Graphing.plot.barh()\n",
    "# Graphing.pie() <- check\n",
    "# Graphing.plot.box() <- check\n",
    "# Graphing.plot.hexbin() <- check\n",
    "# Graphing.plot.kde() <- check\n",
    "# Graphing.plot.density() <- check\n",
    "# print(train_features)\n",
    "# X_full.plot.scatter(x='YearBuilt',y='SalePrice')\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# scatter_matrix(X_full[['YearBuilt','GarageArea','YrSold','SalePrice','TotRmsAbvGrd']],\n",
    "#               alpha=0.2,figsize=(10,8),diagonal='kde') #scatter matrix with density plots in diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration - Train set: Types of features/columns & Missing features\n",
    "#Also needs to be performed for Test set data\n",
    "\n",
    "numbercols = []\n",
    "objectcols = []\n",
    "for col in X_full.columns:\n",
    "    if X_full[col].dtype not in ['object']:\n",
    "        numbercols.append(col)\n",
    "    elif X_full[col].dtype in ['object']:\n",
    "        objectcols.append(col)\n",
    "        \n",
    "missingNum = []\n",
    "missingCat = []\n",
    "\n",
    "# for element in range(len(numbercols)):\n",
    "#     print(numbercols[element],X_full[numbercols[element]].dtype)\n",
    "\n",
    "for col in numbercols:\n",
    "    if X_full[col].count() < X_full.shape[0]:\n",
    "        missingNum.append(col)\n",
    "\n",
    "print(missingNum)\n",
    "\n",
    "for col in objectcols:\n",
    "    if X_full[col].count() < X_full.shape[0]:\n",
    "        missingCat.append(col)\n",
    "\n",
    "print(missingCat)\n",
    "\n",
    "# Create replacement approach for each column - for some imputation, for\n",
    "# others like garages and bedrooms that are optional, can consider inputting\n",
    "# zero or creating a separate column with a flag 1 or 0\n",
    "\n",
    "# Example: Imputation with Median for a certain group\n",
    "# X_full['LotFrontage'] = X_full.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# # Example: Replace Null with 0 = \n",
    "# X_full['GarageYrBlt'] = X_full['GarageYrBlt'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data PreProcessing: Feature Engineering Techniques (numeric)\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575#891273\n",
    "\n",
    "##Feature Profiling (incl BoxCox transformation & visual check)\n",
    "# skewed = X_full[numbercols].apply(lambda x: skew(x)) # To identify high skew columns (>0.5). Can also use boxcox1p & boxcox_normmax\n",
    "skewed[skewed>0.5].index\n",
    "\n",
    "# for col in numbercols: #All NA's must be removed by this point\n",
    "#     X_full[col] = boxcox1p(X_full[col],boxcox_normmax(X_full[col]+1))\n",
    "# plt.figure(figsize=(16,9))\n",
    "# sns.distplot(X_full['LotArea'], kde = False)\n",
    "# sns.scatterplot(x=X_full['LotArea'], y=X_full['SalePrice'])\n",
    "\n",
    "#Remove non-informative columns such as those 1 value (or mostly 1 value) across entire column\n",
    "#as it leads to sparse data set & low quality regression - setting threshold to 90%\n",
    "#Be sure to adjust the numbercols array accordingly to remove toberemoved values\n",
    "toberemoved = []\n",
    "for col in numbercols:\n",
    "    if X_full[col].value_counts().max() > 0.9*X_full.shape[0]:\n",
    "        toberemoved.append(col)\n",
    "print(toberemoved)\n",
    "X_full.drop(toberemoved,axis=1,inplace = True)\n",
    "\n",
    "#Correlation Matrix & HeatMap to address collinearity / correlation\n",
    "a = X_full[numbercols].corr().nlargest(15,\"SalePrice\")[\"SalePrice\"] #.index #Top \"n\" most correlated features\n",
    "print(a)\n",
    "plt.figure(figsize=(10,10)) # Make plot size larger\n",
    "sns.heatmap(X_full[a.index].corr(),vmin=-1,vmax=1,annot=True,cmap='BrBG')\n",
    "plt.savefig('heatmap.png',dpi=300,bbox_inches='tight') #Download as Image\n",
    "# SalePriceCorr = X_full.corr()[['SalePrice']].sort_values(by='SalePrice',ascending=False)\n",
    "# plt.figure(figsize=(2,10))\n",
    "# sns.heatmap(SalePriceCorr,vmin=-1,vmax=1,annot=True)\n",
    "\n",
    "# Model Validation\n",
    "# https://www.xenonstack.com/insights/what-is-model-validation-testing/#:~:text=Validation%20%E2%80%93%20The%20process%20of%20developing,and%20doing%20the%20changes%20accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [i for i in X_train.isnull().columns if X_train[i].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols, axis = 1)\n",
    "reduced_X_valid = X_valid.drop(cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data PreProcessing: Data Cleaning\n",
    "\n",
    "# Convert all train Nominal numeric values to categorical strings, ex. Year\n",
    "X_full['MSSubClass'] = X_full['MSSubClass'].astype('str')\n",
    "X_full['YearBuilt'] = X_full['YearBuilt'].astype('str')\n",
    "X_full['YearRemodAdd'] = X_full['YearRemodAdd'].astype('str')\n",
    "X_full['GarageYrBlt'] = X_full['GarageYrBlt'].astype('str')\n",
    "X_full['MoSold'] = X_full['MoSold'].astype('str')\n",
    "# Convert all test Nominal numeric values to categorical strings, ex. Year\n",
    "X_test_full['MSSubClass'] = X_test_full['MSSubClass'].astype('str')\n",
    "X_test_full['YearBuilt'] = X_test_full['YearBuilt'].astype('str')\n",
    "X_test_full['YearRemodAdd'] = X_test_full['YearRemodAdd'].astype('str')\n",
    "X_test_full['GarageYrBlt'] = X_test_full['GarageYrBlt'].astype('str')\n",
    "X_test_full['MoSold'] = X_test_full['MoSold'].astype('str')\n",
    "X_full.describe(include='all')\n",
    "X_test_full.describe(include='all')\n",
    "\n",
    "# Remove rows without a target, separate target from predictors\n",
    "X_full = X_full.dropna(axis=0, subset=[\"SalePrice\"], inplace = False)\n",
    "y = X_full[\"SalePrice\"]\n",
    "X_full.drop([\"SalePrice\"],axis=1, inplace = True)\n",
    "\n",
    "# # Keeping things simple, not using \"objects\" rather just numeric predictors\n",
    "X = X_full.select_dtypes(exclude=[\"object\"])\n",
    "X_test = X_test_full.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "# # Split validation set from Training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 1)\n",
    "\n",
    "## Imputatation\n",
    "# Option 1: remove all columns with missing values\n",
    "\n",
    "# cols = [i for i in X_train.isnull().columns if X_train[i].isnull().any()]\n",
    "# reduced_X_train = X_train.drop(cols, axis = 1)\n",
    "# reduced_X_valid = X_valid.drop(cols, axis = 1)\n",
    "\n",
    "# Option 2: Impute values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "imputed_X_train.columns = X_train.columns #restore column names\n",
    "imputed_X_valid.columns = X_valid.columns #restore column names\n",
    "\n",
    "# Option 3: Impute values and mark the rows that were imputed in a separate column (ex. houses with blank under garageArea could possibly not have a garage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_missing_value_count_by_column = (X_train.isnull().mean()*100) #percent null\n",
    "missing_value_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_value_count_by_column[missing_value_count_by_column>0])\n",
    "\n",
    "print(X_full[X_full.isnull().values])\n",
    "print(X_full['LotFrontage'].isnull().sum()) #or .any() to check if dataframe has it or not.\n",
    "# X_train[X_train.isnull().values]\n",
    "# X_train['LotFrontage'].isnull().sum()\n",
    "\n",
    "pd.set_option('display.max_columns',None) # to display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Import Libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create Rondom Forest classification model object\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    # Train model\n",
    "    model.fit(X_train,y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid) \n",
    "    return mean_absolute_error(y_valid,prediction)\n",
    "\n",
    "# Model Scoring\n",
    "sk.cross_val_score(model=model,X=X_train,y=y_train,\n",
    "                   scoring=‘neg_mean_absolute_error’,cv=\"\") #scoring strategies here https://scikit-learn.org/stable/modules/model_evaluation.html#scoring # Why is regression listed as negative for all scoring types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is:16657.755650684932\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer is:\" + str(score_dataset(imputed_X_train,y_train,imputed_X_valid,y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is:16367.230342465757\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer is:\" + str(score_dataset(reduced_X_train,y_train,reduced_X_valid,y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models ####Be sure to look at model documentation & experiment with various hyperparameters\n",
    "\n",
    "# Linear Regression\n",
    "# Import Library & other libraries like pandas, numpy etc.\n",
    "from sklearn import linear_model\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Load Train and Test datasets - Identify feature and response variable(s) and values must be numeric and numpy arrays\n",
    "    # Create linear regression object\n",
    "    linear = linear_model.LinearRegression()\n",
    "    # Fit/Train the model using training sets & check score ####Need to check the validation set inclusion through args\n",
    "    linear.fit(X_train, y_train)\n",
    "    linear.score(X_train, y_train)\n",
    "    # Equation coefficient & Intercept\n",
    "    print('Coefficient: \\n',linear.coef_)\n",
    "    print('Intercept: \\n',linear.intercept_)\n",
    "    # Predict Output\n",
    "    predicted = linear.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "# Logistic Regression\n",
    "# Import Library \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create logistic regression object\n",
    "    model = LogisticRegression()\n",
    "    # Train & score the model using training set\n",
    "    model.fit(X_train, y_train)\n",
    "    model.score(X_train, y_train)\n",
    "    # Equation coefficient & intercept\n",
    "    print('Coefficient \\n':model.coef_)\n",
    "    print('Intercept \\n':model.intercept_)\n",
    "    # Predict Output\n",
    "    predicted = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "# Import Library \n",
    "from sklearn import tree\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create tree object\n",
    "    model = tree.DecisionTreeClassifier(criterion='gini') # For classification, you can change the algorithm as gini or entropy (information gain); by default it is Gini\n",
    "    model = tree.DecisionTreeRegressor()\n",
    "    # Train & score the model\n",
    "    model.fit(X_train,y_train)\n",
    "    model.score(X_train,y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "\n",
    "# SVM (Support Vector Machine)\n",
    "# Import Library\n",
    "from sklearn import svm \n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create SVM classification object\n",
    "    model = svm.svc()\n",
    "    # Train & score the model\n",
    "    model.fit(X_train, y_train)\n",
    "    model.score(X_train, y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid) \n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "# Import Libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create NB classification object model\n",
    "    model = GaussianNBare other models for different disbritions: # for continuous values \n",
    "    # there are other models for different distributions:Multinomial and Bernoulli Naive Bayes\n",
    "    # Train model\n",
    "    model.fit(X_train,y_train)\n",
    "    # Prediction Output\n",
    "    prediction = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "# kNN (k-Nearest Neighbors) - Supervised for classification or regression\n",
    "# Import Libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create KNeighbors Classifier object model\n",
    "    model = kNeighborsClassifier(n_neighbors = 6)\n",
    "    # Train model\n",
    "    model.fit(X_train,y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "# k-Means - Unsupervised for clustering\n",
    "# Import Libraries\n",
    "from sklearn.cluster import KMeans\n",
    "# Create k-Means classifier model object\n",
    "model = kMeans(n_clusters=3, random_state=0)\n",
    "# Train model\n",
    "model.fit(X_train)\n",
    "# Predict Output\n",
    "prediction = model.predict(X_valid)\n",
    "\n",
    "# Random Forest Classification\n",
    "# Import Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create Rondom Forest classification model object\n",
    "    model = RandomForestClassifier()\n",
    "    # Train model\n",
    "    model.fit(X_train,y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid) \n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "# Random Forest Regression\n",
    "# Import Libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create Rondom Forest classification model object\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    # Train model\n",
    "    model.fit(X_train,y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid) \n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "# Import Libraries\n",
    "from sklearn import decomposition\n",
    "# The train set contains both X and Y & the test set also contains both X (predictor variables) and Y (target variables)\n",
    "# Create PCA object\n",
    "pca = decomposition.PCA(n_components=k)\n",
    "fa = decomposition.FactorAnalysis()# For factor analysis\n",
    "# Reduce dimensions of training dataset using PCA\n",
    "train_reduced = pca.fit_transform(train)\n",
    "# Reduced the dimension of test set\n",
    "test_reduced = pca.transform(test)\n",
    "\n",
    "# Gradient Boosting & AdaBoost\n",
    "# Import Libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def score_dataset(X_train,y_train,X_valid,y_valid):\n",
    "    # Create Gradient Boosting Classifier object\n",
    "    model = GradientBoostingClassifier.fit()\n",
    "    # Train the model using the training sets\n",
    "    model.fit(X_train,y_train)\n",
    "    # Predict Output\n",
    "    prediction = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid,predicted)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data PreProcessing: Feature Engineering Techniques (Categorical)\n",
    "\n",
    "## Handling NULLS\n",
    "# Option 1: Fill NA with the frequent values/mode of equivalent subgroup\n",
    "# print(X_full['BsmtCond'].value_counts())\n",
    "# X_full['BsmtCond'] = X_full.groupby('OverallCond')['BsmtCond'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "# X_full['BsmtCond'].value_counts()\n",
    "# sns.scatterplot(x=X_full['SaleCondition'],y=X_full['BsmtCond'])\n",
    "# sns.catplot(x='BsmtCond',y='OverallCond',data=X_full)\n",
    "# sns.catplot(x='BsmtCond',y='OverallQual',data=X_full)\n",
    "\n",
    "# Option 2: Fill NA with a predefined value such as 'None' OR their mode\n",
    "# print(X_full['Alley'].value_counts())\n",
    "# X_full['Alley'] = X_full['Alley'].fillna('None')\n",
    "# X_full['Alley'].value_counts()\n",
    "\n",
    "# print(X_full['Electrical'].value_counts())\n",
    "# X_full['Electrical'] = X_full['Electrical'].fillna(X_full['Electrical'].mode()[0])\n",
    "# X_full['Electrical'].value_counts()\n",
    "\n",
    "# Option 3: Drop Column - as long as it doesn't have much predictive power\n",
    "# X = X_train.dtypes==\"object\"\n",
    "# print(X)\n",
    "# drop_X_train = X_train.select_dtypes[exclude='object']\n",
    "# drop_X_valid = X_valid.select_dtypes[exclude='object']\n",
    "\n",
    "# Option 4: Label Encoding - for Categorical variables that are ordinal / on a spectrum\n",
    "# https://www.kaggle.com/alexisbcook/categorical-variables#Three-Approaches\n",
    "\n",
    "# Option 5: One Hot Encoding - For nominal values - generally <15 types for most models\n",
    "# https://www.kaggle.com/alexisbcook/categorical-variables#Three-Approaches\n",
    "\n",
    "##Dropping columns of low value - those with mostly one value\n",
    "# toremovecat = []\n",
    "# for col in objectcols: #find out how many cols mostly (90%) same value\n",
    "#     if X_full[col].value_counts().max() > 0.9*X_full.shape[0]:\n",
    "#         toremovecat.append(col)\n",
    "# print(toremovecat)\n",
    "# X_full.drop(toremovecat,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "##Make Select Categoricals ordinal/numerical - need to remove NaNs prior\n",
    "# to_replace={'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n",
    "# set=['ExterQual', 'ExterCond', 'BsmtQual','BsmtCond', 'HeatingQC',\n",
    "#         'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n",
    "# for col in set:\n",
    "#     X_full[col].replace(to_replace,replacement,inplace=True)\n",
    "\n",
    "# map2 = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "# ['BsmtExposure']\n",
    "\n",
    "# map3 = {'GLQ': 4,'ALQ': 3,'BLQ': 2,'Rec': 3,'LwQ': 2,'Unf': 1,'None': 0}\n",
    "# set3 = ['BsmtFinType1', 'BsmtFinType2']\n",
    "\n",
    "# map4 = {'Y': 1, 'N': 0}\n",
    "# ['CentralAir']\n",
    "\n",
    "# map5 = {'Typ': 3, 'Min1': 2.5, 'Min2': 2, 'Mod': 1.5, 'Maj1': 1, 'Maj2': 0.5, 'Sev': 0, 'Sal': 0}\n",
    "# ['Functional'] \n",
    "\n",
    "##Orginal Categories can be combined such as Garage Quality (times) Garage Condition etc.\n",
    "##if it creates greater impact than having either one separately or both\n",
    "\n",
    "##One-hot encode the rest of the nominal categorical variables using Pandas get_dummies function\n",
    "X_full = pd.get_dummies(X_full).reset_index(drop=True)\n",
    "a=X_full.columns\n",
    "print(a)\n",
    "\n",
    "## Setup Pipeline for preprocessing of numerical & categorical variables\n",
    "# https://www.kaggle.com/alexisbcook/pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
